
import requests
from bs4 import BeautifulSoup
import streamlit as st

# Simulando um navegador
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36',
    'Accept-Language': 'en-US,en;q=0.9,pt-BR;q=0.8,pt;q=0.7',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive'
}


# Fun√ß√£o para verificar se a not√≠cia √© relacionada √† gripe
def is_gripe_related(title, description):
    keywords = ['gripe', 'influenza', 'resfriado', 'covid']
    return any(keyword.lower() in title.lower() or keyword.lower() in description.lower() for keyword in keywords)

# Fun√ß√£o para extrair informa√ß√µes sobre a gripe (site do governo)
def scrape_gripe_info():
    url = 'https://www.gov.br/saude/pt-br/assuntos/saude-de-a-a-z/g/gripe-influenza'
    response = requests.get(url, headers=HEADERS)
    
    if response.status_code != 200:
        raise Exception(f"Erro ao acessar o site: {response.status_code}")
    
    soup = BeautifulSoup(response.content, 'html.parser')
    paragraphs = soup.find_all('p')
    
    gripe_info = [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)]
    return gripe_info


# Fun√ß√£o para coletar not√≠cias da CNN Brasil
def scrape_cnn_news():
    # URL do site
    url = "https://www.cnnbrasil.com.br/tudo-sobre/gripe/"
    
    # Cabe√ßalho para evitar bloqueios
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36"
    }
    
    # Requisi√ß√£o √† p√°gina
    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        raise Exception(f"Erro ao acessar o site: {response.status_code}")
    
    # Parse do conte√∫do HTML
    soup = BeautifulSoup(response.content, "html.parser")
    
    # Selecionar o container principal das not√≠cias
    news_items = soup.find_all("li", class_="home__list__item")
    news_data = []
    
    for item in news_items:
        # Extrair o t√≠tulo
        title_tag = item.find("h3", class_="news-item-header__title")
        title = title_tag.text.strip() if title_tag else "T√≠tulo n√£o encontrado"
        
        # Extrair o link
        link_tag = item.find("a", href=True)
        link = link_tag["href"] if link_tag else "Link n√£o encontrado"
        
        # Extrair a data de publica√ß√£o
        date_tag = item.find("span", class_="home__title__date")
        date = date_tag.text.strip() if date_tag else "Data n√£o encontrada"
        
        # Filtrar por relev√¢ncia (gripe)
        if "gripe" or "influeza" or "resfriado" or "covid" in title.lower():
            news_data.append({
                "title": title,
                "link": link,
                "description": title,  # A descri√ß√£o √© igual ao t√≠tulo
                "date": date
            })
    
    return news_data

# Fun√ß√£o para coletar not√≠cias do G1
def scrape_g1_news(state, city=None):
    search_query = f"gripe {state}" + (f" {city}" if city else "")
    url = f"https://g1.globo.com/busca/?q={search_query}"
    response = requests.get(url, headers=HEADERS)

    if response.status_code != 200:
        raise Exception(f"Erro ao acessar a p√°gina do G1 (Status code: {response.status_code})")
    
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # Selecionando os elementos de not√≠cia
    news_elements = soup.find_all('div', class_='widget--info__text-container')
    if not news_elements:
        return []

    news_data = []
    for element in news_elements:
        title_tag = element.find('div', class_='widget--info__title')
        link_tag = element.find('a', href=True)
        description_tag = element.find('p', class_='widget--info__description')
        date_tag = element.find('div', class_='widget--info__meta')

        if title_tag and link_tag:
            title = title_tag.text.strip()
            link = link_tag['href']
            description = description_tag.text.strip() if description_tag else "Descri√ß√£o n√£o dispon√≠vel"
            date = date_tag.text.strip() if date_tag else "Data n√£o informada"

            # Aplicar filtro por dengue
            if is_gripe_related(title, description):
                news_data.append({
                    'title': title,
                    'link': link,
                    'description': description,
                    'date': date
                })

    return news_data

# Fun√ß√£o para exibir as not√≠cias no Streamlit
def show_news(news, title):
    st.subheader(title)
    if not news:
        st.write("Nenhuma not√≠cia encontrada.")
        return
    
    for item in news:
        st.markdown(f"**[{item['title']}]({item['link']})**")
        st.write(f"*Publicado em: {item['date']}*")
        st.write(f"{item['description']}")
        st.write("---")

# Streamlit App
def noticias_informacoes_gripe():
    st.title("üîç Not√≠cias e Informa√ß√µes sobre gripe üîç")
    
    # Informa√ß√µes gerais
    if st.button("Carregar Informa√ß√µes sobre gripe"):
        try:
            informacoes = scrape_gripe_info()
            for info in informacoes:
                st.write(info)
        except Exception as e:
            st.error(f"Erro ao carregar informa√ß√µes: {e}")
    
    # Not√≠cias gerais da CNN
    if st.button("Carregar Not√≠cias Gerais"):
        try:
            cnn_news = scrape_cnn_news()
            show_news(cnn_news, "Not√≠cias Gerais sobre gripe - CNN Brasil")
        except Exception as e:
            st.error(f"Erro ao carregar not√≠cias gerais: {e}")
    
    # Not√≠cias por estado e munic√≠pio
    from user_global import MUNICIPIO_USUARIO_GRIPE
    
    if st.button("Carregar Not√≠cias por Munic√≠pio"):
        try:
           
            city_news = scrape_g1_news(MUNICIPIO_USUARIO_GRIPE) if MUNICIPIO_USUARIO_GRIPE else []
            
            
            show_news(city_news, f"Not√≠cias no Munic√≠pio: {MUNICIPIO_USUARIO_GRIPE}")
        except Exception as e:
            st.error(f"Erro ao carregar not√≠cias por estado ou munic√≠pio: {e}")
