import requests
from bs4 import BeautifulSoup
import streamlit as st
from pydantic import BaseModel


# Simulando um navegador
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36',
    'Accept-Language': 'pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive'
}

# Valida√ß√£o de input para a ferramenta
class NoticiaDengueInput(BaseModel):
    estado: str
    municipio: str

# Fun√ß√£o auxiliar para verificar relev√¢ncia sobre dengue
def is_dengue_related(title, description):
    keywords = ['dengue', 'zika', 'chikungunya', 'aedes aegypti', 'febre amarela']
    return any(keyword.lower() in title.lower() or keyword.lower() in description.lower() for keyword in keywords)

# Fun√ß√£o para extrair informa√ß√µes sobre a dengue (site do governo)
def scrape_dengue_info():
    url = 'https://www.gov.br/saude/pt-br/assuntos/saude-de-a-a-z/d/dengue'
    response = requests.get(url, headers=HEADERS)
    
    if response.status_code != 200:
        raise Exception(f"Erro ao acessar o site: {response.status_code}")
    
    soup = BeautifulSoup(response.content, 'html.parser')
    paragraphs = soup.find_all('p')
    
    dengue_info = [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)]
    return dengue_info


# Fun√ß√£o para coletar not√≠cias da CNN Brasil
def scrape_cnn_news():
    # URL do site
    url = "https://www.cnnbrasil.com.br/tudo-sobre/dengue/"
    
    # Cabe√ßalho para evitar bloqueios
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36"
    }
    
    # Requisi√ß√£o √† p√°gina
    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        raise Exception(f"Erro ao acessar o site: {response.status_code}")
    
    # Parse do conte√∫do HTML
    soup = BeautifulSoup(response.content, "html.parser")
    
    # Selecionar o container principal das not√≠cias
    news_items = soup.find_all("li", class_="home__list__item")
    news_data = []
    
    for item in news_items:
        # Extrair o t√≠tulo
        title_tag = item.find("h3", class_="news-item-header__title")
        title = title_tag.text.strip() if title_tag else "T√≠tulo n√£o encontrado"
        
        # Extrair o link
        link_tag = item.find("a", href=True)
        link = link_tag["href"] if link_tag else "Link n√£o encontrado"
        
        # Extrair a data de publica√ß√£o
        date_tag = item.find("span", class_="home__title__date")
        date = date_tag.text.strip() if date_tag else "Data n√£o encontrada"
        
        # Filtrar por relev√¢ncia (dengue)
        if "dengue" or "zika" or "chikungunya" or "aedes aegypty" in title.lower():
            news_data.append({
                "title": title,
                "link": link,
                "description": title,  # A descri√ß√£o √© igual ao t√≠tulo
                "date": date
            })
    
    return news_data

def sanitize_input(text):
    """Remove caracteres indesejados e normaliza o texto."""
    return text.strip().title()

# Fun√ß√£o para coletar not√≠cias do G1
def scrape_g1_news(state, city=None):
    state = state.strip().title()
    city = city.strip().title() if city else None
    search_query = f"dengue {state}" + (f" {city}" if city else "")
    url = f"https://g1.globo.com/busca/?q={search_query}"
    response = requests.get(url, headers=HEADERS)

    if response.status_code != 200:
        raise Exception(f"Erro ao acessar a p√°gina do G1 (Status code: {response.status_code})")

    soup = BeautifulSoup(response.content, 'html.parser')
    news_elements = soup.find_all('div', class_='widget--info__text-container')
    if not news_elements:
        return []

    news_data = []
    for element in news_elements[:5]:  # Limite de 5 not√≠cias
        title_tag = element.find('div', class_='widget--info__title')
        link_tag = element.find('a', href=True)
        description_tag = element.find('p', class_='widget--info__description')
        date_tag = element.find('div', class_='widget--info__meta')

        if title_tag and link_tag:
            title = title_tag.text.strip()
            link = link_tag['href']
            description = description_tag.text.strip() if description_tag else "Descri√ß√£o n√£o dispon√≠vel"
            date = date_tag.text.strip() if date_tag else "Data n√£o informada"

            if is_dengue_related(title, description):
                news_data.append({
                    "title": title,
                    "link": link,
                    "description": description,
                    "date": date
                })

    return news_data

# Fun√ß√£o para exibir as not√≠cias no Streamlit
def show_news(news, title):
    st.subheader(title)
    if not news:
        st.write("Nenhuma not√≠cia encontrada.")
        return
    
    for item in news:
        st.markdown(f"**[{item['title']}]({item['link']})**")
        st.write(f"*Publicado em: {item['date']}*")
        st.write(f"{item['description']}")
        st.write("---")


# Streamlit App
def exibir_noticias_informacoes():
    st.title("üîç Not√≠cias e Informa√ß√µes sobre Dengue üîç")
    
    # Informa√ß√µes gerais
    if st.button("Carregar Informa√ß√µes sobre Dengue"):
        try:
            informacoes = scrape_dengue_info()
            for info in informacoes:
                st.write(info)
        except Exception as e:
            st.error(f"Erro ao carregar informa√ß√µes: {e}")
    
    # Not√≠cias gerais da CNN
    if st.button("Carregar Not√≠cias Gerais"):
        try:
            cnn_news = scrape_cnn_news()
            show_news(cnn_news, "Not√≠cias Gerais sobre Dengue - CNN Brasil")
        except Exception as e:
            st.error(f"Erro ao carregar not√≠cias gerais: {e}")
    
    # Not√≠cias por estado e munic√≠pio
    from user_global import ESTADO_USUARIO, MUNICIPIO_USUARIO
    
    if st.button("Carregar Not√≠cias por Estado e Munic√≠pio"):
        try:
            state_news = scrape_g1_news(ESTADO_USUARIO)
            city_news = scrape_g1_news(ESTADO_USUARIO, MUNICIPIO_USUARIO) if MUNICIPIO_USUARIO else []
            
            show_news(state_news, f"Not√≠cias no Estado: {ESTADO_USUARIO}")
            show_news(city_news, f"Not√≠cias no Munic√≠pio: {MUNICIPIO_USUARIO}")
        except Exception as e:
            st.error(f"Erro ao carregar not√≠cias por estado ou munic√≠pio: {e}")
